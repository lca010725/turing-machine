# turing-machine
something about  data science
# 互联网情感分析-Bert模型
关键词：文本情感极性分类，Bert模型
## 摘要
 情感分类是对带有感情色彩的主观性文本进行分析、推理的过程,即分析说话人的态度,推断其所包含的情感类别.传统机器学习在处理情感分类问题的时候通常是基于SVM、CRF、信息熵等传统算法,其优势在于具有对多种特征建模的能力,但要用人工标注的单个词作为特征，而语料的不足往往就是性能的瓶颈.对句子进行情感分类的难处在于如何抽取到句子中与情感表达紧密相关的特征,以人工标注的单个词作为特征会忽略单词所处的上下文语义信息,导致最终的分类效果不理想.为了解决特征抽取的难题,使用2018年Google提出的文本预训练模型Bert(bidirectional encoder representations from transformer)来挑战这次比赛中的情感多分类任务.Bert利用transformer超强的特征抽取能力来学习词语的双向编码表示,融合了上下文信息的词语编码能更好地进行情感决策.实验结果也表明使用Bert的效果要比使用传统算法的baseline好很多。
## 赛题背景&任务
随着各种社交平台的兴起，网络上用户的生成内容越来越多，产生大量的文本信息，如新闻、微博、博客等，面对如此庞大且富有情绪表达的文本信息，完全可以考虑通过探索他们潜在的价值为人们服务。因此近年来情绪分析受到计算机语言学领域研究者们的密切关注，成为一项进本的热点研究任务。

本题目标为在庞大的数据集中精准的区分文本的情感极性，情感分为正中负三类。面对浩如烟海的新闻信息，精确识别蕴藏在其中的情感倾向，对舆情有效监控、预警及疏导，对舆情生态系统的良性发展有着重要的意义。

**任务：**

需要对提供的新闻数据进行情感极性分类，其中正面情绪对应0，中性情绪对应1以及负面情绪对应2。根据提供的训练数据，通过算法或模型判断出测试集中新闻的情感极性。

## Bert模型
Google提出的BERT模型可用于问答系统，情感分析，垃圾邮件过滤，命名实体识别，文档聚类等任务中，作为这些任务的基础设施即语言模型。
### 模型提出
短信在现代人的通讯中被广泛使用,大量的微表情被用来替代帮助人们更好得表达自己的情感.然而,对于对微表情不熟悉的人来说,找到他想要的那个表情并不是一件简单的事(也许你可以考虑下你的父母以及爷爷奶奶如何使用这些微表情).因此,很有必要设计一个系统来基于短信的内容自动推荐合适的微表情.之前看Google I/O大会看到Google做的功能类似的一款产品,通过识别短信内容来提供给用户以微表情替代情感相关文本的选项,感觉很有趣.这项任务其实就是NLP领域经典的情感分类问题,通过对短信作情感分析来为其匹配一个最合适的微表情类.对这些短信进行情感分类的难点在于: 1.反讽问题,比如"你牛逼你上啊"; 2.领域相关的问题,"我的电脑散热声音很大"、"我家洗衣机声音很大"这些很可能是差评,而"我家音响声音很大"很可能就是好评; 3.网络流行语也会影响情感分析,比如"给力"、"不明觉厉"、"累觉不爱"、"细思极恐"等,这些词利用传统的分词一般都会被切开,而且会影响词性标注,如果想避免只能加入人工干预,修改分词的粒度和词性标注的结果; 4.文本比较短,省略较严重,导致出现歧义或指代错误等,比如"咬死猎人的狗".传统的统计+规则的方法不能很好得解决这些难点,需要引入深度学习强大的特征抽取能力.2018年10月份,Google公司提出了NLP集大成者Bert模型[1] .这个模型既引入了lstm的双向编码机制同时还采用了GPT中的Transformer来做特征抽取,具有非常强大的文本特征提取能力,能学习到句子中潜在的句法和语义信息.除此之外,Bert基于character-level做embedding,就不存在分词以及测试集包含训练集中未出现词的困扰了.这些优点使得Bert能够比较好得解决情感分类问题中的一些难点,实验基于Google开源的Bert预训练好的中文模型做fine-tuning,最终的实验效果要比采用传统方法得到得baseline好很多.

### 相关工作
现有研究已经产生了可用于情感分析多项任务的大量技术,包括监督和无监督方法.在监督方法中,早期论文使用所有监督机器学习方法(如支持向量机、最大熵、朴素贝叶斯等)和特征组合.无监督方法包括使用情感词典、语法分析和句法模式的不同方法.大约十年前,深度学习成为强大的机器学习技术,在很多应用领域产生了当前最优的结果,包括计算机视觉、语音识别、NLP 等.近期将深度学习应用到情感分析也逐渐变得流行.
![Bert和其它深度学习模型之间的关系](https://i.loli.net/2019/12/07/A4xHu8KN1cBCRqG.png)
情感分析可划分为3种粒度:文档粒度,句子粒度,短语粒度.这次的实验任务主要是基于句子粒度来进行情感分类.Kim等人于2013年提出的CNN文本分类工作[2] ,成为句子级情感分类任务的重要baseline之一.基本的lstm模型加上pooling策略构成分类模型,也通常用来做句子级情感分析的方法.Tang等人于2015年发表的工作[3] 使用两种不同的RNN网络,结合文本和主题进行情感分析.这几年情感分析方面的突破主要都集中在深度学习领域,深度学习通过学习文本编码表示来抽取文本深层次的特征,解决传统方法无法很好地学习到文本特征的难题.文献[4,8]是从2013年至2018年期间深度学习在文本特征抽取方面的几项重大成就,其中包括Word2Vec,GLoVe,Transformer,ELMo,GPT.而本次实验将采用Bert模型[1] 是这几项工作的集大成者,如上图1所示是Bert与ELMo等深度学习模型之间的关系,它结合了Transformer和ELMo的优点,相比LSTM能较好地解决上下文长距离依赖问题,学习到了句子的句法特征以及深层次的语义特征,具有更强的特征抽取能力.相比传统的机器学习方法,效果要好很多.

### Bert原理简述
BERT 的创新点在于它将双向 Transformer 用于语言模型，
之前的模型是从左向右输入一个文本序列，或者将 left-to-right 和 right-to-left 的训练结合起来。
实验的结果表明，**双向训练的语言模型对语境的理解会比单向的语言模型更深刻**，
论文中介绍了一种新技术叫做 Masked LM（MLM），在这个技术出现之前是无法进行双向语言模型训练的。

BERT 利用了 Transformer 的 encoder 部分。
Transformer 是一种注意力机制，可以学习文本中单词之间的上下文关系的。
Transformer 的原型包括两个独立的机制，一个 encoder 负责接收文本作为输入，一个 decoder 负责预测任务的结果。
BERT 的目标是生成语言模型，所以只需要 encoder 机制。

Transformer 的 encoder 是一次性读取整个文本序列，而不是从左到右或从右到左地按顺序读取，
这个特征使得模型能够基于单词的两侧学习，相当于是一个双向的功能。

下图是 Transformer 的 encoder 部分，输入是一个 token 序列，先对其进行 embedding 称为向量，然后输入给神经网络，输出是大小为 H 的向量序列，每个向量对应着具有相同索引的 token。
![20191208134919.png](https://i.loli.net/2019/12/08/9XIuEPCfVn3aqcM.png)

当我们在训练语言模型时，有一个挑战就是要定义一个预测目标，很多模型在一个序列中预测下一个单词，
“The child came home from ___”
双向的方法在这样的任务中是有限制的，为了克服这个问题，**BERT 使用两个策略**:

#### 1. Masked LM (MLM)
在将单词序列输入给 BERT 之前，每个序列中有 15％ 的单词被 [MASK] token 替换。 然后模型尝试基于序列中其他未被 mask 的单词的上下文来预测被掩盖的原单词。

这样就需要：

1. 在 encoder 的输出上添加一个分类层
2. 用嵌入矩阵乘以输出向量，将其转换为词汇的维度
3. 用 softmax 计算词汇表中每个单词的概率
BERT 的损失函数只考虑了 mask 的预测值，忽略了没有掩蔽的字的预测。这样的话，模型要比单向模型收敛得慢，不过结果的情境意识增加了。
![20191208135037.png](https://i.loli.net/2019/12/08/L4gWxHC8AedmK5b.png)

#### 2. Next Sentence Prediction (NSP)
在 BERT 的训练过程中，模型接收成对的句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子。
在训练期间，50％ 的输入对在原始文档中是前后关系，另外 50％ 中是从语料库中随机组成的，并且是与第一句断开的。

为了帮助模型区分开训练中的两个句子，输入在进入模型之前要按以下方式进行处理：

在第一个句子的开头插入 [CLS] 标记，在每个句子的末尾插入 [SEP] 标记。
将表示句子 A 或句子 B 的一个句子 embedding 添加到每个 token 上。
给每个 token 添加一个位置 embedding，来表示它在序列中的位置。
为了预测第二个句子是否是第一个句子的后续句子，用下面几个步骤来预测：

1. 整个输入序列输入给 Transformer 模型
2. 用一个简单的分类层将 [CLS] 标记的输出变换为 2×1 形状的向量
3. 用 softmax 计算 IsNextSequence 的概率
在训练 BERT 模型时，Masked LM 和 Next Sentence Prediction 是一起训练的，目标就是要最小化两种策略的组合损失函数。
![20191208135155.png](https://i.loli.net/2019/12/08/4XyBYFOcUf3uxh7.png)


### 如何使用 BERT?
BERT 可以用于各种NLP任务，只需在核心模型中添加一个层，例如：

1. 在分类任务中，例如情感分析等，只需要在 Transformer 的输出之上加一个分类层
2. 在问答任务（例如SQUAD v1.1）中，问答系统需要接收有关文本序列的 question，并且需要在序列中标记 answer。 可以使用 BERT 学习两个标记 answer 开始和结尾的向量来训练Q＆A模型。
3. 在命名实体识别（NER）中，系统需要接收文本序列，标记文本中的各种类型的实体（人员，组织，日期等）。 可以用 BERT 将每个 token 的输出向量送到预测 NER 标签的分类层。
![20191208135315.png](https://i.loli.net/2019/12/08/ANy9pWOREP8LkUM.png)


## 模型运用结果
![TIM截图20191203153746.png](https://i.loli.net/2019/12/08/h4dOkLVr6gXM1UK.png)
该赛题的评分方式为Macro-F1评分方式，即召回率和准确率的倒数和的倒数的两倍，其中召回率表示正确预测阳性的样本占实际阳性样本的比例，准确率表示预测阳性样本中正确预测的比例。实际输出的.csv文件会附在压缩包中。


## 参考文献
[1]: Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deepbidirectional transformers for language understanding. CoRR, 2018,abs/1810.04805.
[2]: Zhang L, Wang S, Liu B. Deep learning for sentiment analysis: A survey[J]. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 2018, 8(4): e1253.
[4]: Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-tations in vector space. arXiv preprint arXiv, 2013,1301.3781.
[8]: Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving languageunderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018.
